# -*- coding: utf-8 -*-

import ast
import concurrent.futures
import hashlib
import itertools
import json
import logging
import os
import threading
import time
import pytz
from datetime import datetime
#from functools import reduce
from telegram import Bot

import requests
import urllib3

try:
    import win_unicode_console  # pipå®‰è£…
    win_unicode_console.enable()  # è§£å†³VSCode on Windowsçš„è¾“å‡ºå¼‚å¸¸é—®é¢˜
except:
    pass  # ä¸è£…è¿™ä¸ªåŒ…ï¼Œåªè¦ä¸åœ¨VScodeä¸‹è¿è¡Œä¹Ÿä¸ä¸€å®šæœ‰é—®é¢˜


class wannengError(Exception):  # ç»ƒä¹ å†™ä¸ªå¼‚å¸¸ï¼Ÿ
    def __init__(self, value):
        self.value = value

    def __str__(self):
        return repr(self.value)


class threadsafe_generator():  # è§£å†³å¤šä¸ªçº¿ç¨‹åŒæ—¶æŠ¢å ä¸€ä¸ªç”Ÿæˆå™¨å¯¼è‡´çš„é”™è¯¯ï¼Œä¹–ä¹–æ’é˜Ÿå»å§æ‚¨ä»¬
    def __init__(self, gen):
        self.gen = gen
        self.lock = threading.Lock()

    def __iter__(self):
        return next(self)

    def next(self):
        with self.lock:
            return next(self.gen)

class MapTypeHelper():
    ''' åŒ¹é…ä¸åŒåœ°å›¾çš„URLè·¯å¾„å‚æ•°
    Working on : Overviewer, Mapcrafter
    
    '''
    def __init__(self, name):
        self.name = name
    
    #def

    #map_type = MapTypeHelper(self.getMapType)


class counter():
    def __init__(self):
        self.a = {'404': 0, 'Fail': 0, 'Ignore': 0, 'Added': 0,
                  'Update': 0, 'Replace': 0, 'unModded': 0}

    def plus(self, str):
        self.a[str] += 1

    def __str__(self):
        str = ''
        if self.a['Added'] != 0:
            str += 'Added:\t{}\n'.format(self.a['Added'])
        str += 'Update:\t{}\nUnmodded:\t{}\nIgnore:\t{}\n404:\t{}\nFail:\t{}\n'.format(
            self.a['Update'], self.a['unModded'], self.a['Ignore'], self.a['404'], self.a['Fail'])
        if self.a['Replace'] != 0:
            str += 'Replace:\t{}\n'.format(self.a['Replace'])
        return str

class crawler():
    def __init__(self, config, noFetch=False):
        """
        åˆå§‹åŒ–å›¾ç‰‡æŠ“å–å™¨

        ä»é…ç½®æ–‡ä»¶è¯»å–å„ç§è·¯å¾„

        Args: 
            config (dict): See ./config_example.json for detail.
            noFetch (bool): Skip fetchTotalDepth() when set to True. 
                            Save your time when not interacting with overviewer map.                    
        """
        '''æ–‡ä»¶/è·¯å¾„è®¾ç½®'''
        '''ä¸€ä¸ªæ­£ç¡®çš„é“¾æ¥,overviewerç‰ˆ https://map.nyaacat.com/kedama/v2_daytime/0/3/3/3/3/3/3/2/3/2/3/1.jpg?c=1510454854'''
        '''mapcrafterç‰ˆï¼š https://map.nyaacat.com/kedama/v3_daytime/tl/3/2/2/2/2/2/4.jpg'''
        self.map_domain = config['map_domain']  # Overvieweråœ°å›¾åœ°å€
        self.map_name = config['map_name']  # åœ°å›¾åç§°
        
        self.map_savename = self.map_name if 'map_savename' not in config else config['map_savename']
        
        self.image_folder = 'images/{}'.format(self.map_savename)  # å›¾å—å­˜å“ª
        self.data_folder = 'data/{}'.format(self.map_savename)  # æ›´æ–°å†å²å­˜å“ªï¼ˆä»¥åå‡çº§æ•°æ®åº“ï¼Ÿï¼‰
        self.log_folder = 'log/{}'.format(self.map_savename)  # æ—¥å¿—æ–‡ä»¶å¤¹

        os.environ['TZ'] = 'Asia/Shanghai' #ä¿ç•™è¿™è¡Œ æ¯•ç«Ÿåœ¨Linuxé‡Œè¿˜ä¼šç”¨ï¼Œèƒ½è®©æ—¥å¿—æ—¥æœŸæ­£ç¡®ã€‚
        self.today = datetime.now(pytz.timezone('Asia/Shanghai')).strftime('%Y%m%d')
        self.logger = self.makeLogger()  # æ—¥å¿—è®°å½•å™¨
        self.timestamp = str(int(time.time()))  # è¯·æ±‚å›¾å—è¦ç”¨ï¼Œæ—¶åŒºæ— å…³
        self.logger.debug('Today is set to {}'.format(self.today))

        '''æŠ“å–è®¾ç½®'''
        self.map_type = self.getMapType() if noFetch == False else config['latest_renderer'] #æ¸²æŸ“å™¨ç§ç±»
        self.map_rotation = config['map_rotation'] if 'map_rotation' in config else 'tl'
        self.max_threads = config['max_crawl_threads']  # æœ€å¤§æŠ“å›¾çº¿ç¨‹æ•°
        # ç¼©æ”¾çº§åˆ«æ€»æ•°
        self.total_depth = config['last_total_depth'] if noFetch == True else self.fetchTotalDepth(
        )
        # ç›®æ ‡å›¾å—çš„ç¼©æ”¾çº§åˆ«,ä»0å¼€å§‹ï¼Œæ¯æ‰©å¤§è§‚å¯ŸèŒƒå›´ä¸€çº§-1ã€‚
        self.target_depth = config['target_depth']
        # è¿½è¸ªå˜è¿å†å²çš„åŒºåŸŸï¼Œ [((0, -8), 56, 29)] for  v1/v2 on Kedama server
        self.crawl_zones = ast.literal_eval(config['crawl_zones'])

    def getMapType(self):
        """ç¡®å®šåœ°å›¾ç§ç±»
        
        æ¢æµ‹åœ°å›¾ç«™ç‚¹ä¸­ï¼Œä¸ç‰¹å®šæ¸²æŸ“å™¨ç›¸å…³çš„jsæ–‡ä»¶ï¼ˆåï¼‰ï¼Œä»è€Œå¾—çŸ¥å®ƒä»¬ç”¨äº†ä»€ä¹ˆæ¸²æŸ“å™¨ã€‚
        https://map.example.com/kedama/static/js/mapcrafterui.js
        https://ob-mc.net/build/overviewer.js

        Input: none 
        Return: str: 'Overviewer' or 'Mapcrafter' "
        
        """
        renderer_names = {'Mapcrafter' : 'static/js/mapcrafterui.js', 'Overviewer' : 'overviewerConfig.js'}
        errors = 0
        
        for renderer_name in renderer_names:
            URL = '{}/{}'.format(self.map_domain, renderer_names[renderer_name])
            print(URL)
            try:
                r = requests.head(URL, timeout=5)
                if r.status_code == 404:
                    self.logger.debug('The renderer is not {}'.format(renderer_name))
                elif r.status_code == 200:
                    self.logger.info('The renderer is {}.'.format(renderer_name))
                    return renderer_name
                else:#å¼‚å¸¸å¤„ç†ï¼Ÿ
                    raise wannengError(r.status_code)
            except Exception as e:
                self.logger.error('Err {} : {}'.format(errors, e))
                errors += 1
                if errors >= 5:
                    raise e

        #If the code reaches here, no renderer is found, raise an exception.
        raise wannengError("No known renderer found.")
        

    def makeLogger(self):
        """ç”³è¯·å¹¶é…ç½®æŠ“å–å™¨æ‰€ç”¨åˆ°çš„æ—¥å¿—è®°å½•å™¨

        è‹¥æ—¥å¿—æ–‡ä»¶å¤¹ä¸å­˜åœ¨å°†è¢«åˆ›å»ºï¼Œå‘ç»ˆç«¯è¾“å‡ºé•¿åº¦è¾ƒçŸ­çš„æ–‡æœ¬ï¼Œå‘æ—¥å¿—æ–‡ä»¶å†™å…¥å®Œæ•´é•¿åº¦çš„æŠ¥å‘Š

        Todo: æ¯æ¬¡æŠ“å–æ‘˜è¦ï¼šæ—¶é—´ï¼ŒæŠ“å–åœ°å›¾ï¼ŒæŠ“å–ç»“æœç»Ÿè®¡[ï¼Œå­˜å‚¨é…é¢å‰©ä½™å®¹é‡]
        
        Args: None

        Returns: instance of `logging`"""
        logger = logging.getLogger(self.map_savename)
        logger.setLevel(logging.DEBUG)
        log_path = '{}/{}.log'.format(self.log_folder, self.today)  # æ–‡ä»¶å
        if not os.path.exists(self.log_folder):  # åˆæ¬¡è¿è¡Œï¼Œåˆ›å»ºlogæ–‡ä»¶å¤¹
            os.makedirs(self.log_folder)
            print('Made directory\t./'+self.log_folder)
        fh = logging.FileHandler(log_path)
        ch = logging.StreamHandler()
        fh.setLevel(logging.DEBUG)
        ch.setLevel(logging.DEBUG)
        datefmt_ch = '%H:%M:%S'  # è¾“å‡ºæ¯«ç§’è¦æ”¹loggingçš„ä»£ç ï¼Œä½ æƒ³æ¸…æ¥šå°±å¥½ã€‚
        fmt_fh = '[%(asctime)s]-[%(levelname).1s:%(funcName)-20.15s] %(message)s'
        # å±å¹•è¾“å‡ºç›¸å¯¹ç®€çŸ­
        fmt_ch = '[%(asctime)s.%(msecs)03d]-[%(levelname).1s:%(funcName).6s] %(message).60s'
        fh.setFormatter(logging.Formatter(fmt_fh))
        ch.setFormatter(logging.Formatter(fmt_ch, datefmt_ch))
        logger.addHandler(fh), logger.addHandler(ch)
        return logger

    def downloadImage(self, URL):
        """ä¸‹è½½ç»™å®šURLçš„æ–‡ä»¶å¹¶è¿”å›

        åˆ«çæ”¹ï¼Œæ”¹äº†(1)æ¬¡åˆæ”¹å›æ¥äº†

        Args: 
            URL (str): URL of the image which is going to be downloaded.

        Returns:
            dict: The response headers of the given URL, the image object.

        Raises:
            requests.exceptions.ReadTimeout,
            requests.exceptions.ConnectionError,
            urllib3.exceptions.ReadTimeoutError
            
            Let the caller function handle these errors."""

        r = requests.get(URL, stream='True', timeout=5)
        if r.status_code == 200:
            img = r.raw.read()
            return {'headers': r.headers, 'image': img}

    def makePath(self, zoneLists, depth):
        """ç”Ÿæˆç»™å®šè§‚å¯ŸåŒºåŸŸçš„å›¾å—è·¯å¾„ã€‚

        ä»å·¦åˆ°å³å¤„ç†å„åˆ—ï¼Œä»ä¸Šåˆ°ä¸‹ç”Ÿæˆæ¯åˆ—ä¸­çš„pathã€‚

        Args: eg. : [((0, -8), 56, 29)] , -3
            zonelists (list of tuple): contains a list of zones that we are watching.
                zones (tuple): (center_X,center_Y), width, height
                    center_X, center_Y (int): The center of a watch zone.
                    width (int): The horizontal image-block numbers at the given zoom depth.
                    height (int) : The vertical image-block numbers at the given zoom depth.
            depth (int): target zoom depth, should be a negative number."""

        for center, width, height in zoneLists:  # å¼€å§‹å¯¹ç»™å®šçš„åŒºåŸŸ**ä¹‹ä¸€**ç”Ÿæˆåæ ‡
            X_list = [X for X in range(center[0]-width*2**-depth, center[0] +
                                       width*2**-depth) if (X / (2**-depth)) % 2 == 1]
            Y_list = [Y for Y in range(center[1]+height*2**-depth, center[1] -
                                       height*2**-depth, -1) if (Y / (2**-depth)) % 2 == 1]
            for XY in itertools.product(X_list, Y_list):  # æ±‚ä¸¤ä¸ªåˆ—è¡¨çš„ç¬›å¡å°”ç§¯
                yield self.xy2Path(XY, self.total_depth)

    def xy2Path(self, XY, depth):
        """ç”±å›¾å—åæ ‡ç”Ÿæˆå›¾å—è·¯å¾„
        
        Args: Eg. : (12,4) , 4
            XY (int,int): The X and Y coordinates for the project-defined coordinate system. 
            depth (int): The total zoom-levels for the given overviewer map.
        
        Returns: Eg. : '/0/3/3/3/1/2/1/3'
            path(str): Path of the img block.  """

        X = XY[0]
        Y = XY[1]  # æœŸæœ›åæ ‡å€¼
        table = ['/2', '/0', '/3', '/1']
        val_X = 0
        val_Y = 0  # æœ¬æ¬¡è¿­ä»£åçš„åæ ‡å€¼
        p = depth  # å½“å‰å¤„ç†çš„ç¼©æ”¾ç­‰çº§
        path = ''  # è¿”å›å€¼çš„åˆå€¼
        while (val_X != X) and (val_Y != Y):  # æœªè¿­ä»£åˆ°æœŸæœ›åæ ‡ç‚¹ï¼šä¾æ¬¡è®¡ç®—æ¨ªçºµåæ ‡ä¸‹ä¸€å±‚æ˜¯å“ªå—
            p -= 1
            #01|11  |----\  0|1
            #00|10  |----/  2|3
            tmp_X = 0 if val_X > X else 1
            val_X += (2 * tmp_X - 1) * (2 ** p)
            tmp_Y = 0 if val_Y > Y else 1
            val_Y += (2 * tmp_Y - 1) * (2 ** p)
            tmp = tmp_X * 2 + tmp_Y
            path += table[tmp]
        return path

    def path2xy(self, path, depth):
        """ç”±å›¾å—è·¯å¾„è½¬é¡¹ç›®å®šä¹‰çš„åæ ‡ã€‚
        
        ä¼ å‚æ—¶åº”ä¿è¯pathä¸ç»™å®šçˆ¬å›¾ç­‰çº§ç›¸ç¬¦ï¼Œå³ path = total_depth + target_depth 
        
        Args: E.g. : '/0/3/3/3/1/2/1/3' , 15
            path (int) : The overviewer img block path to convert.
                P.S.: The '/' in the beginning is needed
            depth (int) : The total zoom-levels for the given overviewer map."""

        in_list = map(int, path.split('/')[1:])
        X, Y = (0, 0)
        table = [1, 3, 0, 2]
        for index, value in enumerate(in_list):
            X += (table[value]//2-0.5)*2**(depth-index)  # éœ€è¦æ•´æ•°é™¤
            Y += (table[value] % 2-0.5)*2**(depth-index)
        return(int(X), int(Y))

    '''
    è‹¥å—ç½‘ç»œç­‰å½±å“æœªè·å–åˆ°å€¼ï¼Œåˆ™æ•´ä¸ªè„šæœ¬é€€å‡ºã€‚'''

    def fetchTotalDepth(self):
        """é€å±‚çˆ¬å–å›¾å—ï¼Œæ¢æµ‹ç»™å®šçš„Overvieweråœ°å›¾ä¸€å…±å¤šå°‘å±‚

        æŒ‰ç…§å®ƒä»¬ç”Ÿæˆåœ°å›¾çš„è§„åˆ™ï¼Œç¡¬ç¼–ç å–æœ€é è¿‘åœ°å›¾åæ ‡åŸç‚¹å³ä¸Šæ–¹çš„å›¾å—ï¼Œä¾‹å¦‚ /1 /1/2 /1/2/2 â€¦â€¦(Overviewerç‰ˆæœ¬)
        è¯¥å‡½æ•°è¿”å›crawlerç±»åˆå§‹åŒ–æ‰€éœ€çš„å‚æ•°ï¼Œè‹¥å‘ç”Ÿå¼‚å¸¸åˆ™è„šæœ¬æ–‡ä»¶åº”å½“é€€å‡ºï¼Œè€Œä¸èƒ½å‘ä¸‹æ‰§è¡Œã€‚

        Args: None

        Returns:
            depth (int): The total zoom-levels for the given overviewer map."""

        errors = 0            
        configPaths = {'Overviewer' : 'overviewerConfig.js', 'Mapcrafter' : 'config.js'}
        path = '/1'
        depth = 0
        while True:  # do-while å¾ªç¯ç»“æ„çš„ä¸€ç§æ”¹å†™
            
            URL = {'Overviewer' : '{}/{}{}.jpg?c={}'.format(self.map_domain,self.map_name, path, self.timestamp),
                   'Mapcrafter' : '{}/{}/{}{}.jpg'  .format(self.map_domain,self.map_name, self.map_rotation, path)}

            try:
                print('.', end='', flush=True)  # åªè¾“å‡ºï¼Œä¸æ¢è¡Œï¼Œè¾¹çˆ¬è¾¹è¾“å‡ºã€‚
                r = requests.head(URL[self.map_type], timeout=5)
                if r.status_code == 404:
                    break
                elif r.status_code == 200:
                    depth += 1
                    direction_num = {'Overviewer' : '/2', 'Mapcrafter' : '/4'}
                    path = path+direction_num[self.map_type]
                    errors = 0
                else:
                    raise wannengError(r.status_code)
            except Exception as e:
                self.logger.error('Err {} : {}'.format(errors, e))
                errors += 1
                if errors >= 5:
                    raise e
        print()
        self.logger.info("Total zoom depth: {}".format(depth))
        return depth

    def changeImgName(self):
        '''å°†ä¸Šä¸€ä»£pathå‘½åçš„æ–‡ä»¶åå’Œæ›´æ–°è®°å½•è½¬æ¢ä¸ºâ€˜ç¼©æ”¾çº§åˆ«_æ¨ªåæ ‡_çºµåæ ‡.jpgâ€™ï¼Œåªç”¨æ¥æ‰¹é‡é‡å‘½åè€ç‰ˆæœ¬è„šæœ¬ä¸‹è½½çš„å›¾ç‰‡
        
        å‡½æ•°å·²åºŸå¼ƒ'''
        #å…ˆæ”¹å›¾ç‰‡åï¼Œå†æ”¹å†å²è®°å½•
        prevwd = os.getcwd()
        os.chdir(self.image_folder)
        for dir in os.listdir():
            os.chdir(dir)
            time.sleep(3)
            for filename in os.listdir():
                path = filename.split('.')[0].replace('_', '/')
                XY = self.path2xy(path, 11)
                new_file_name = str(self.target_depth) + \
                    '_'+str(XY[0])+'_'+str(XY[1])+'.jpg'
                os.rename(filename, new_file_name)
                print(filename, '-->', new_file_name)
            os.chdir('..')
        os.chdir(prevwd)
        print('changing back to', os.getcwd())

    def changeJsonKey(self):
        '''å‡çº§ _æ›´æ–°å†å²_æ–‡ä»¶
        
        è¯¥å‡½æ•°å·²åºŸå¼ƒ'''
        with open(self.data_folder+'/'+'update_history.json', 'r') as f:
            log_buffer = json.load(f)
            new_log_buffer = {}
            for origin_key in log_buffer.keys():
                path = origin_key.split('.')[0].replace('_', '/')
                XY = self.path2xy(path, 11)
                new_key = str(self.target_depth)+'_' + \
                    str(XY[0])+'_'+str(XY[1])+'.jpg'
                new_log_buffer[new_key] = log_buffer[origin_key]
            with open(self.data_folder+'/'+'update_history.json', 'w') as f:  # å†™å› å›¾å—æ›´æ–°å²æ–‡ä»¶
                json.dump(new_log_buffer, f, indent=2)

    def getImgdir(self, dir):
        """è¿”å›ä¿å­˜è¯¥åœ°å›¾ä»Šæ—¥æ›´æ–°äº†çš„å›¾å—çš„æ–‡ä»¶å¤¹åœ°å€
        
        ä¿å­˜æ¯å¼ å›¾ç‰‡å‰éƒ½ä¼šæ£€æŸ¥ï¼Œè‹¥æ–‡ä»¶å¤¹ä¸å­˜åœ¨å°†è¢«åˆ›å»ºã€‚

        Args: Eg. : (.)'/images/v2_daytime'
            dir(str) : Where all images are saved.

        Yields: Eg. : (.)'/images/v2_daytime/20180202/'
            new_dir (str) : Where to save the images being crawled today."""

        new_dir = '{}/{}/'.format(dir, self.today)
        while(True):
            #time.sleep(0.03)  # ä¸æƒ³è¾“å‡ºå¤ªå¿«
            if not os.path.exists(new_dir):
                os.makedirs(new_dir)
                self.logger.info('Made directory\t./{}'.format(new_dir))
            yield new_dir

    '''ï¼Œï¼Œä¸€è½®å®Œæˆè€Œä¸æ˜¯å…ˆheadå†get'''       

    def runsDaily(self):
        """æ¯æ—¥è¿è¡Œçš„æŠ“å›¾å­˜å›¾å‡½æ•°ï¼Œä»¥å•ä¸ªovervieweråœ°å›¾ä¸ºèŒƒå›´ï¼ŒæŠ“å–å¹¶æ›´æ–°åº“ä¸­çš„å›¾ç‰‡ã€‚
        
        ç¬¬ä¸€æ¬¡è¿è¡Œåˆ›å»ºè·¯å¾„å’Œæ•°æ®æ–‡ä»¶ï¼Œå…¨é‡ä¸‹/å­˜å›¾ç‰‡ï¼Œ
        åç»­æ¯æ¬¡åªä¸‹è½½ETagå˜åŠ¨çš„å›¾ç‰‡å¹¶ä¿å­˜å…¶ä¸­SHA1å˜åŠ¨çš„å›¾ç‰‡(çº¦å å‰è€…çš„1/3?)"""

        bot = Bot(token = "508665684:AAH_vFcSOrXiIuGnVBc-xi0A6kPl1h7WFZc" )

        statistics = counter() # ç»Ÿè®¡æŠ“å›¾çŠ¶æ€
        update_history = {}  # æ›´æ–°å†å²
        latest_ETag = {} # æ¯ä¸ªåŒºå—çš„æœ€æ–°ETag

        # è¯»å–å›¾å—æ›´æ–°å²ï¼Œè‹¥æ–‡ä»¶ä¸å­˜åœ¨åˆ™è¿å¸¦æ‰€è¿°ç›®å½•ä¸€åŒåˆ›å»ºã€‚
        try:
            with open('{}/update_history.json'.format(self.data_folder), 'r') as f:
                update_history = json.load(f)

            with open('{}/latest_ETag.json'.format(self.data_folder), 'r') as f:
               
                    latest_ETag = json.load(f)
        except FileNotFoundError:
                if not os.path.exists(self.data_folder):
                    os.makedirs(self.data_folder)
                    self.logger.info(
                        'Made directory\t./{}'.format(self.data_folder))

        to_crawl = self.makePath(
            self.crawl_zones, self.target_depth)  # ç”Ÿæˆè¦æŠ“å–çš„å›¾ç‰‡åæ ‡
        save_in = self.getImgdir(self.image_folder)
        save_in = threadsafe_generator(save_in)

        def addNewImg(path, URL, file_name):
            """å‘æ–‡ä»¶ç³»ç»Ÿå’Œæ›´æ–°å†å²è®°å½•ä¸­æ·»åŠ æ–°å›¾ç‰‡
            
            Args: 
                URL (str): The url of a specific image.
                file_name (str): What to save the img as.
            Returns:
                ret_msg (str): The log message of the very image."""
            response = self.downloadImage(URL)
            update_history[file_name] = (
                [{'Save_in': save_in.next(), 'ETag': response['headers']['ETag']}])
            with open(save_in.next()+file_name, 'wb') as f:
                f.write(response['image'])
                f.close()
            ret_msg = 'Add\t{}.jpg as {}'.format(path, file_name)
            return ret_msg

        def processBySHA1(URL, response, file_name):
            """ä¸‹è½½å›¾å—å¹¶æ ¹æ®æ‘˜è¦æ¥å¤„ç†æ–‡ä»¶
            
            é€‚ç”¨äºç«™ç‚¹æœ€æ–°å›¾ç‰‡å’Œæœ¬åœ°ä¿å­˜çš„æœ€æ–°å›¾ç‰‡ETagä¸åŒçš„æ—¶å€™
            
            Args:
                URL (str) : The url of a specific image.
                response : The response of head(url).
                file_name (str): What to save the img as.
            
            Returns:
                ret_msg (str): The log message of the very image."""

            DL_img = self.downloadImage(URL)['image']
            In_Stock_Latest = update_history[file_name][-1]['Save_in'] + file_name
            with open(In_Stock_Latest, 'rb') as Prev_img:
                # SHA1ä¸ä¸€è‡´ï¼Œå–»ç¤ºå›¾ç‰‡å‘ç”Ÿäº†å®è´¨æ€§ä¿®æ”¹
                if hashlib .sha1(Prev_img .read()) .hexdigest() != hashlib .sha1(DL_img) .hexdigest():
                    # åŒä¸€å¤©å†…ä¸¤æ¬¡æŠ“åˆ°çš„å›¾ç‰‡å‘ç”Ÿäº†åå·®ï¼Œæ›¿æ¢æ‰æœ¬åœ°åŸæ¥çš„æœ€æ–°å›¾ç‰‡å’Œæ›´æ–°è®°å½•
                    if update_history[file_name][-1]['Save_in'] == save_in.next():
                        del update_history[file_name][-1]
                        statistics.plus('Replaced')
                        ret_msg = 'Rep\t{}'.format(file_name)
                    else:
                        statistics.plus('Update')
                        ret_msg = 'Upd\t{}'.format(file_name)
                    update_history[file_name].append(
                        {'Save_in': save_in.next(), 'ETag': response.headers['ETag']})
                    with open(save_in.next()+file_name, 'wb') as f:
                            f.write(DL_img)
                            f.close()
                else:
                    # SHA1ä¸€è‡´ï¼Œå›¾ç‰‡æ— å®è´¨æ€§å˜åŒ–ï¼Œåˆ™å¿½ç•¥è¯¥ä¸åŒ
                    statistics.plus('unModded')
                    ret_msg = 'nMOD\t{}'.format(file_name)
                return ret_msg

        def visitPath(path):  #
            """æŠ“å–å•å¼ å›¾ç‰‡å¹¶å¯¹å“åº”è¿›è¡Œå¤„ç†çš„å·¥äºº
            
            å¯¹æ¯å¼ å›¾ç‰‡è¿›è¡Œæœ€å¤š5æ¬¡ä¸‹è½½å°è¯•ï¼Œå¦‚æœè¿˜æ˜¯ä¸‹ä¸æ¥å°±æ”¾å¼ƒè¿™å¼ å›¾ç‰‡
            
            Args: Eg. : /0/3/3/3/3/3/3/2/3/2/3/1
                path (str): The overviewer img block path to download.

            Returns:
                ret_msg (str): The log message of the very image."""

            URL = '{}/{}{}.jpg?c={}'.format(self.map_domain,
                                            self.map_name, path, self.timestamp)
            tryed_time = 0
            while True:
                visitpath_status = 'none'
                try:
                    r = requests.head(URL, timeout=5)
                    
                    # 404--å›¾å—ä¸å­˜åœ¨
                    if r.status_code == 404:
                        statistics.plus('404')
                        ret_msg = '404\t{}'.format(path)
                    # 200--OK
                    elif r.status_code == 200:
                        XY = self.path2xy(path, self.total_depth)
                        file_name = '{}_{}_{}.jpg'.format(
                            self.target_depth, XY[0], XY[1])
                        visitpath_status = 'filename set'
                        # åº“é‡Œæ— è¯¥å›¾--Add
                        if file_name not in update_history:
                            visitpath_status = 'To add img'
                            statistics.plus('Added')
                            ret_msg = addNewImg(path, URL, file_name)
                            latest_ETag[file_name] = {'ETag' : r.headers['ETag']}
                            visitpath_status = 'img added'
                        # åº“é‡Œæœ‰è¯¥å›¾ç‰‡
                        else:
                            # ETagä¸ä¸€è‡´--ä¸¢ç»™ä¸‹ä¸€çº§å¤„ç†
                            try:
                                if r.headers['ETag'] != latest_ETag[file_name]['ETag']: 
                                    # BUG: ğŸ‘†latest_etag ä¸­æ²¡æœ‰éƒ¨åˆ†å›¾å—ï¼Œè€Œupdate_historyé‡Œå´æœ‰ã€‚
                                    # è¿™æ˜¯ç”±äºé‚£äº›å›¾å—å‡åœ¨åœ°å›¾è¾¹ç¼˜ä¸”latest_etagä½œä¸ºç‹¬ç«‹æ–‡ä»¶å»ºç«‹è¾ƒæ™šï¼Œ
                                    # å»ºç«‹åå›¾å—å°±ä¸€ç›´æ²¡æ›´æ–°äº†ã€‚
                                    # å»ºè®®åˆ é™¤update_historyä¸­çš„é‚£äº›å›¾å—å¹¶æ ¡éªŒä¸¤ä¸ªæ•°æ®æ–‡ä»¶ä¸­çš„é”®ä¸€è‡´æ€§ã€‚
                                    visitpath_status = 'ETag inconsistent'
                                    ret_msg = processBySHA1(URL, r, file_name)
                                # ETagä¸€è‡´--åªå‡ºä¸ªlog
                                else:
                                    visitpath_status = 'ETag consistent'
                                    statistics.plus('Ignore')
                                    ret_msg = 'Ign\t{}'.format(file_name)
                            except KeyError: 
                                # update_historyä¸­çš„éƒ¨åˆ†å›¾å—é”®åœ¨latest_etagä¸­æ²¡æœ‰ï¼Œæ˜¯å†å²é—ç•™é—®é¢˜ã€‚
                                # åœ¨è¿™catchæ‰å¼‚å¸¸ï¼Œåé¢ä¸€è¡Œä»£ç å¥½æ·»åŠ æ­£ç¡®çš„etagã€‚
                                self.logger.error('{} don\'t show up in latest_ETag but shows in '.format(path))
                                latest_ETag[file_name] = {'ETag' : update_history[file_name][-1]['ETag']}
                                self.logger.error('Copied ETag from update_history to latest_ETag for {}'.format(file_name))
                        latest_ETag[file_name]['ETag'] = r.headers['ETag']
                    return ret_msg
                except (KeyboardInterrupt) as e:
                    raise e
                # ç½‘ç»œé‡åˆ°é—®é¢˜ï¼Œé‡è¯•æœ€å¤š5æ¬¡
                except Exception as e:
                    self.logger.error(
                        'No.{} for\t{}\t{}'.format(tryed_time, path, e))
                    self.logger.error(visitpath_status)
                    tryed_time += 1
                    if tryed_time >= 5:
                        statistics.plus('Fail')
                        ret_msg = 'Fail\t{}'.format(path)
                        return ret_msg

        # ç»´æŠ¤ä¸€ä¸ªæŠ“å›¾çº¿ç¨‹æ± 
        # Todo: å¤ç”¨æŠ“å›¾ç½‘ç»œè¿æ¥ï¼Œå‡å°‘å…¨ç¨‹å‘å‡ºçš„è¿æ¥æ•°
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_threads) as executor:
            try:
                for msg in executor.map(visitPath, to_crawl):
                    self.logger.warn(
                        msg) if 'Fail' in msg or 'Rep' in msg else self.logger.info(msg)
            except KeyboardInterrupt:
                self.logger.warn('User pressed ctrl+c.')
                self.logger.warn('Will exit when other threads return.')
                return 0

        # å°†ä»Šå¤©çš„æŠ“å›¾æƒ…å†µå†™å›æ›´æ–°å†å²æ–‡ä»¶
        self.logger.debug('Start dumping json at {}'.format(time.time()))
        with open('{}/update_history.json'.format(self.data_folder), 'w') as f:
            json.dump(update_history, f, indent=2, sort_keys=True)
            self.logger.debug('update_history dumped at {}'.format(time.time()))
        with open('{}/latest_ETag.json'.format(self.data_folder), 'w') as f:
            json.dump(latest_ETag,f,indent=2, sort_keys=True)
            self.logger.debug('latest_ETag dumped at {}'.format(time.time()))
        try:
            bot.send_message(176562893,'Crawl result {} for {} : \n{}'.format(self.today,self.map_name,str(statistics)))
        except Exception :
            self.logger.warning('Telegram bot failed sending {} statistics!'.format(self.map_name))

def main():
    try:
        with open('config.json', 'r+') as f:
            configs = json.load(f)
            for map_name in configs.keys():
                if configs[map_name]['enable_crawl'] == True:
                    cr = crawler(configs[map_name], noFetch=False)
                    cr.runsDaily()
                    if configs[map_name]['last_total_depth'] != cr.total_depth:
                        configs[map_name]['last_total_depth'] = cr.total_depth
                else: 
                    print("skipping map {}".format(map_name))
            f.seek(0)
            json.dump(configs, f, indent=2, sort_keys=True)
            f.truncate()
    except Exception as e:
        print(e)        
        with open('log/errors.txt','a+') as f:
            print(str(e),file = f)
        bot = Bot(token = "508665684:AAH_vFcSOrXiIuGnVBc-xi0A6kPl1h7WFZc" )
        bot.send_message(176562893,'Something went wrong, see logs/errors.txt for detail')



if __name__ == '__main__':
    main()
