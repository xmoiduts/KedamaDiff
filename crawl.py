# -*- coding: utf-8 -*-

import ast
import concurrent.futures
import hashlib
import itertools
import json
import logging
import os
import threading
import time
import pytz
from datetime import datetime
#from functools import reduce
from telegram import Bot
from configs.config import CrawlerConfig as CrConf
from configs.crawl_list import CrawlList as map_list

import requests
import urllib3
import asyncio
import aiohttp
import sqlite3

try:
    import win_unicode_console  # pipå®‰è£…
    win_unicode_console.enable()  # è§£å†³VSCode on Windowsçš„è¾“å‡ºå¼‚å¸¸é—®é¢˜
except:
    pass  # ä¸è£…è¿™ä¸ªåŒ…ï¼Œåªè¦ä¸åœ¨VScodeä¸‹è¿è¡Œä¹Ÿä¸ä¸€å®šæœ‰é—®é¢˜


class wannengError(Exception):  # ç»ƒä¹ å†™ä¸ªå¼‚å¸¸ï¼Ÿ
    def __init__(self, value):
        self.value = value

    def __str__(self):
        return repr(self.value)


class threadsafe_generator():  # è§£å†³å¤šä¸ªçº¿ç¨‹åŒæ—¶æŠ¢å ä¸€ä¸ªç”Ÿæˆå™¨å¯¼è‡´çš„é”™è¯¯ï¼Œä¹–ä¹–æ’é˜Ÿå»å§æ‚¨ä»¬
    def __init__(self, gen):
        self.gen = gen
        self.lock = threading.Lock()

    def __iter__(self):
        return next(self)

    def next(self):
        with self.lock:
            return next(self.gen)

class MapTypeHelper():
    ''' åŒ¹é…ä¸åŒåœ°å›¾çš„URLè·¯å¾„å‚æ•°
    Working on : Overviewer, Mapcrafter
    
    '''
    def __init__(self, name):
        self.name = name
    
    #def

    #map_type = MapTypeHelper(self.getMapType)


class counter():
    def __init__(self):
        self.a = {'404': 0, 'Fail': 0, 'Ignore': 0, 'Added': 0,
                  'Update': 0, 'Replace': 0, 'unModded': 0}

    def plus(self, str):
        self.a[str] += 1

    def __str__(self):
        str = ''
        if self.a['Added'] != 0:
            str += 'Added:\t{}\n'.format(self.a['Added'])
        str += 'Update:\t{}\nUnmodded:\t{}\nIgnore:\t{}\n404:\t{}\nFail:\t{}\n'.format(
            self.a['Update'], self.a['unModded'], self.a['Ignore'], self.a['404'], self.a['Fail'])
        if self.a['Replace'] != 0:
            str += 'Replace:\t{}\n'.format(self.a['Replace'])
        return str

class crawler():
    # é’ˆå¯¹(å•å¼ åœ°å›¾,å•çº§ç¼©æ”¾)çš„å›¾å—æŠ“å–å™¨
    def __init__(self, config, noFetch=False): # TODO: config -> map_config
        """
        åˆå§‹åŒ–å›¾ç‰‡æŠ“å–å™¨

        ä»é…ç½®æ–‡ä»¶è¯»å–å„ç§è·¯å¾„

        Args: 
            config (dict): See ./config_example.json for detail.
            noFetch (bool): Skip fetchTotalDepth() when set to True. 
                            Save your time when not interacting with overviewer map.                    
        """
        '''æ–‡ä»¶/è·¯å¾„è®¾ç½®'''
        '''ä¸€ä¸ªæ­£ç¡®çš„é“¾æ¥,overviewerç‰ˆ https://map.nyaacat.com/kedama/v2_daytime/0/3/3/3/3/3/3/2/3/2/3/1.jpg?c=1510454854'''
        '''mapcrafterç‰ˆï¼š https://map.nyaacat.com/kedama/v3_daytime/tl/3/2/2/2/2/2/4.jpg'''
        self.map_domain = config.map_domain  # Overvieweråœ°å›¾åœ°å€
        self.map_name = config.map_name  # åœ°å›¾åç§°
        
        self.map_savename = self.map_name if 'map_savename' not in config else config.map_savename
        
        self.image_folder = '{}/images/{}'.format(CrConf.data_folders, self.map_savename)  # å›¾å—å­˜å“ª
        self.data_folder  = '{}/data/{}'  .format(CrConf.data_folders, self.map_savename)  # æ›´æ–°å†å²å­˜å“ªï¼ˆä»¥åå‡çº§æ•°æ®åº“ï¼Ÿï¼‰
        self.log_folder  =  '{}/log/{}'   .format(CrConf.data_folders, self.map_savename)  # æ—¥å¿—æ–‡ä»¶å¤¹

        os.environ['TZ'] = CrConf.timezone #ä¿ç•™è¿™è¡Œ æ¯•ç«Ÿåœ¨Linuxé‡Œè¿˜ä¼šç”¨ï¼Œèƒ½è®©æ—¥å¿—æ—¥æœŸæ­£ç¡®ã€‚
        self.today = datetime.now(pytz.timezone(CrConf.timezone)).strftime('%Y%m%d')
        self.logger = self.makeLogger()  # æ—¥å¿—è®°å½•å™¨
        self.timestamp = str(int(time.time()))  # è¯·æ±‚å›¾å—è¦ç”¨ï¼Œæ—¶åŒºæ— å…³
        self.logger.debug('Today is set to {}'.format(self.today))

        '''æŠ“å–è®¾ç½®'''
        self.map_type = self.getMapType() if noFetch == False else config.latest_renderer #æ¸²æŸ“å™¨ç§ç±» # TODO: æœ€æ–°æ¸²æŸ“å™¨ç§ç±»ä¸¢åˆ°æ•°æ®åº“å»æˆ–å–æ¶ˆæ‰
        self.map_rotation = config.map_rotation if 'map_rotation' in config else 'tl'
        self.max_threads = config.max_crawl_workers  # æœ€å¤§æŠ“å›¾çº¿ç¨‹æ•°
        # ç¼©æ”¾çº§åˆ«æ€»æ•° TODO: ä»æ•°æ®åº“ä¸­å–æ•°
        self.total_depth = config.last_total_depth if noFetch == True else self.fetchTotalDepth(
        )
        # ç›®æ ‡å›¾å—çš„ç¼©æ”¾çº§åˆ«,ä»0å¼€å§‹ï¼Œæ¯æ‰©å¤§è§‚å¯ŸèŒƒå›´ä¸€çº§-1ã€‚
        self.target_depth = config.target_depth
        # è¿½è¸ªå˜è¿å†å²çš„åŒºåŸŸï¼Œ [((0, -8), 56, 29)] for  v1/v2 on Kedama server
        self.crawl_zones = ast.literal_eval(config.crawl_zones)
        self.dry_run = False # In dry-run mode, neither commit DB nor save image file, while logs are permitted.

    def getMapType(self):
        """ç¡®å®šåœ°å›¾ç§ç±»
        
        æ¢æµ‹åœ°å›¾ç«™ç‚¹ä¸­ï¼Œä¸ç‰¹å®šæ¸²æŸ“å™¨ç›¸å…³çš„jsæ–‡ä»¶ï¼ˆåï¼‰ï¼Œä»è€Œå¾—çŸ¥å®ƒä»¬ç”¨äº†ä»€ä¹ˆæ¸²æŸ“å™¨ã€‚
        https://map.example.com/kedama/static/js/mapcrafterui.js
        https://ob-mc.net/build/overviewer.js

        Input: none 
        Return: str: 'Overviewer' or 'Mapcrafter' "
        
        """
        renderer_names = {'Mapcrafter' : 'static/js/mapcrafterui.js', 'Overviewer' : 'overviewerConfig.js'}
        errors = 0
        
        for renderer_name in renderer_names:
            URL = '{}/{}'.format(self.map_domain, renderer_names[renderer_name])
            print(URL)
            try:
                r = requests.head(URL, timeout = CrConf.crawl_request_timeout)
                if r.status_code == 404:
                    self.logger.debug('The renderer is not {}'.format(renderer_name))
                elif r.status_code == 200:
                    self.logger.info('The renderer is {}.'.format(renderer_name))
                    return renderer_name
                else:#å¼‚å¸¸å¤„ç†ï¼Ÿ
                    raise wannengError(r.status_code)
            except Exception as e:
                self.logger.error('Err {} : {}'.format(errors, e))
                errors += 1
                if errors >= CrConf.crawl_request_retries:
                    raise e

        #If the code reaches here, no renderer is found, raise an exception.
        raise wannengError("No known renderer found.")
        

    def makeLogger(self):
        """ç”³è¯·å¹¶é…ç½®æŠ“å–å™¨æ‰€ç”¨åˆ°çš„æ—¥å¿—è®°å½•å™¨

        è‹¥æ—¥å¿—æ–‡ä»¶å¤¹ä¸å­˜åœ¨å°†è¢«åˆ›å»ºï¼Œå‘ç»ˆç«¯è¾“å‡ºé•¿åº¦è¾ƒçŸ­çš„æ–‡æœ¬ï¼Œå‘æ—¥å¿—æ–‡ä»¶å†™å…¥å®Œæ•´é•¿åº¦çš„æŠ¥å‘Š

        Todo: æ¯æ¬¡æŠ“å–æ‘˜è¦ï¼šæ—¶é—´ï¼ŒæŠ“å–åœ°å›¾ï¼ŒæŠ“å–ç»“æœç»Ÿè®¡[ï¼Œå­˜å‚¨é…é¢å‰©ä½™å®¹é‡]
        
        Args: None

        Returns: instance of `logging`"""
        logger = logging.getLogger(self.map_savename)
        logger.setLevel(logging.DEBUG)
        log_path = '{}/{}.log'.format(self.log_folder, self.today)  # æ–‡ä»¶å
        if not os.path.exists(self.log_folder):  # åˆæ¬¡è¿è¡Œï¼Œåˆ›å»ºlogæ–‡ä»¶å¤¹
            os.makedirs(self.log_folder)
            print('Made directory\t./'+self.log_folder)
        fh = logging.FileHandler(log_path)
        ch = logging.StreamHandler()
        fh.setLevel(logging.DEBUG)
        ch.setLevel(logging.DEBUG)
        datefmt_ch = '%H:%M:%S'  # è¾“å‡ºæ¯«ç§’è¦æ”¹loggingçš„ä»£ç ï¼Œä½ æƒ³æ¸…æ¥šå°±å¥½ã€‚
        fmt_fh = '[%(asctime)s]-[%(levelname).1s:%(funcName)-20.15s] %(message)s'
        # å±å¹•è¾“å‡ºç›¸å¯¹ç®€çŸ­
        fmt_ch = '[%(asctime)s.%(msecs)03d]-[%(levelname).1s:%(funcName).6s] %(message).60s'
        fh.setFormatter(logging.Formatter(fmt_fh))
        ch.setFormatter(logging.Formatter(fmt_ch, datefmt_ch))
        logger.addHandler(fh), logger.addHandler(ch)
        return logger

    def prepareDBConnection(self):
        # Connects to DB and return its connection,
        # Create DB file path if not exist
        # Create DB Tables and headers on DB file creation.
        try:
            sqliteConnection = sqlite3.connect(
                '{}/crawl_records.db'.format(self.data_folder))
        except sqlite3.OperationalError:
            self.logger.warning(
                'DB not found, creating its directory: ')
            self.logger.warning(
                '{}/crawl_records.db'.format(self.data_folder))
            if not os.path.exists(self.data_folder):
                os.makedirs(self.data_folder)
                self.logger.info(
                    'Made directory\t./{}'.format(self.data_folder)
                    )
            sqliteConnection = sqlite3.connect(
                '{}/crawl_records.db'.format(self.data_folder))
        self.logger.info(
            'Connected to {}/crawl_records.db'.format(self.data_folder))

        # init tables with headers.
        cursor = sqliteConnection.cursor()
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS crawl_records(
                file_name varchar(100) NOT NULL,
                crawled_at varchar(8) NOT NULL,
                map_rotation varchar(2), 
                ETag varchar(30) NOT NULL, 
                zoom_level INTEGER NOT NULL,
                coord_x INTEGER NOT NULL,
                coord_y INTEGET NOT NULL,
                stored_at varchar(255),
                frozen boolean DEFAULT False,
                deleted boolean DEFAULT False
	        )'''
        )
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS map_attributes(
                id INTEGER PRIMARY KEY NOT NULL,
                map_name varchar(30),
                data_path varchar(128),
                last_total_depth INTEGER,
                last_update varchar(8)
            )''' # This table only 1 row, id only == 1
        )

        return sqliteConnection

    def updateDBDates(self):
        # update 'last_update' in DB.map_attributes .
        cursor = self.sqliteConnection.cursor()
        cursor.execute('''
            INSERT OR REPLACE INTO map_attributes
                (id, map_name, data_path, last_total_depth, last_update)
            VALUES (?,?,?,?,?)''',
            (1, 
            self.map_name, 
            CrConf.data_folders,
            self.total_depth,
            self.today
            )
        )

    def getLatestSavedETag(self, file_name) -> str: 
        # Return the latest saved ETag 
        #    of the given file_name from DB;
        # If the filename has no record then return ''.
        retrieved = self.getLatest('ETag', file_name)
        return '' if retrieved is None else retrieved[0]
    
    def getLatestUpdatePath(self, file_name) -> str:
        # Return path of the latest crawl record
        #    of the given file_name from DB;
        # example: 'images/v1_daytime/20180228/' for -3_184_-200.jpg
        # If filename has no record then return ''.
        retrieved = self.getLatest('stored_at', file_name)
        return '' if retrieved is None else retrieved[0]

    def getLatestUpdateDate(self, file_name) -> str:
        # Return the latest update date 
        #    of the given file_name from DB;
        # example: '20201112'
        retrieved = self.getLatest('ETag', file_name) # ETag is not used in this function
        # ... but essential to make a correct function call.
        return '' if retrieved is None else retrieved[1]

    def getLatest(self, item, file_name):
        # Return the latest item from DB
        # If item has no record then return ''.
        # TODO: Filter out `deleted = True` records.
        cursor = self.sqliteConnection.cursor()
        cursor.execute('''
            SELECT {}, crawled_at
            FROM crawl_records
            WHERE file_name = ? AND deleted IS "False"
            ORDER BY crawled_at DESC
            LIMIT 1
        '''.format(item), (file_name,) 
        )
        retrieved = cursor.fetchone() 
        return retrieved

    def deactivateCrawlRecord(self, file_name, date):
        # Soft delete a record in crawl record DB
        # accroadign to given filename and date.
        # show warning message if row(s) other than 1 is affected.
        cursor = self.sqliteConnection.cursor()
        cursor.execute('''
            UPDATE crawl_records
            SET deleted = "True"
            WHERE file_name = ?
                AND crawled_at = ?
                AND deleted IS "False"
        ''',(file_name, date) 
        )
        if cursor.rowcount != 1:
            self.logger.warning("Deactivated {} lines on {}, {}".format(cursor.rowcount, file_name, date))
        else:
            self.logger.info("Deactivated {} lines on {}, {}".format(cursor.rowcount, file_name, date))
        return 

    def addCrawlRecord(self, file_name, date, ETag, zoom_level, coord_x, coord_y, path):
        cursor = self.sqliteConnection.cursor()
        cursor.execute('''
            INSERT INTO crawl_records
            (file_name, crawled_at, ETag, zoom_level, coord_x, coord_y, stored_at)
            VALUES (?,?,?,?,?,?,?)''',
            (file_name, date, ETag, zoom_level, coord_x, coord_y, path) 
        )

    async def downloadImage(self, sess, URL):
        """ä¸‹è½½ç»™å®šURLçš„æ–‡ä»¶å¹¶è¿”å›

        åˆ«çæ”¹ï¼Œæ”¹äº†(1)æ¬¡åˆæ”¹å›æ¥äº†

        Args: 
            URL (str): URL of the image which is going to be downloaded.

        Returns:
            dict: The response headers of the given URL, the image object.

        Raises:
            requests.exceptions.ReadTimeout,
            requests.exceptions.ConnectionError,
            urllib3.exceptions.ReadTimeoutError
            
            Let the caller function handle these errors."""

        '''
        r = requests.get(URL, stream='True', timeout=5)
        if r.status_code == 200:
            img = r.raw.read()
            return {'headers': r.headers, 'image': img}
        '''
        async with sess.get(URL, timeout = 5) as response: 
            if response.status == 200:
                img = await response.read()
            return {'headers': response.headers, 'image': img}


    def makePath(self, zoneLists, depth):
        """ç”Ÿæˆç»™å®šè§‚å¯ŸåŒºåŸŸçš„å›¾å—è·¯å¾„ã€‚

        ä»å·¦åˆ°å³å¤„ç†å„åˆ—ï¼Œä»ä¸Šåˆ°ä¸‹ç”Ÿæˆæ¯åˆ—ä¸­çš„pathã€‚

        Args: eg. : [((0, -8), 56, 29)] , -3
            zonelists (list of tuple): contains a list of zones that we are watching.
                zones (tuple): (center_X,center_Y), width, height
                    center_X, center_Y (int): The center of a watch zone.
                    width (int): The horizontal image-block numbers at the given zoom depth.
                    height (int) : The vertical image-block numbers at the given zoom depth.
            depth (int): target zoom depth, should be a negative number."""

        for center, width, height in zoneLists:  # å¼€å§‹å¯¹ç»™å®šçš„åŒºåŸŸ**ä¹‹ä¸€**ç”Ÿæˆåæ ‡
            X_list = [X for X in range(center[0]-width*2**-depth, center[0] +
                                       width*2**-depth) if (X / (2**-depth)) % 2 == 1]
            Y_list = [Y for Y in range(center[1]+height*2**-depth, center[1] -
                                       height*2**-depth, -1) if (Y / (2**-depth)) % 2 == 1]
            for XY in itertools.product(X_list, Y_list):  # æ±‚ä¸¤ä¸ªåˆ—è¡¨çš„ç¬›å¡å°”ç§¯
                yield self.xy2Path(XY, self.total_depth)

    def xy2Path(self, XY, depth):
        """ç”±å›¾å—åæ ‡ç”Ÿæˆå›¾å—è·¯å¾„
        
        Args: Eg. : (12,4) , 4
            XY (int,int): The X and Y coordinates for the project-defined coordinate system. 
            depth (int): The total zoom-levels for the given overviewer map.
        
        Returns: Eg. : '/0/3/3/3/1/2/1/3'
            path(str): Path of the img block.  """

        X = XY[0]
        Y = XY[1]  # æœŸæœ›åæ ‡å€¼
        table = ['/2', '/0', '/3', '/1']
        val_X = 0
        val_Y = 0  # æœ¬æ¬¡è¿­ä»£åçš„åæ ‡å€¼
        p = depth  # å½“å‰å¤„ç†çš„ç¼©æ”¾ç­‰çº§
        path = ''  # è¿”å›å€¼çš„åˆå€¼
        while (val_X != X) and (val_Y != Y):  # æœªè¿­ä»£åˆ°æœŸæœ›åæ ‡ç‚¹ï¼šä¾æ¬¡è®¡ç®—æ¨ªçºµåæ ‡ä¸‹ä¸€å±‚æ˜¯å“ªå—
            p -= 1
            #01|11  |----\  0|1
            #00|10  |----/  2|3
            tmp_X = 0 if val_X > X else 1
            val_X += (2 * tmp_X - 1) * (2 ** p)
            tmp_Y = 0 if val_Y > Y else 1
            val_Y += (2 * tmp_Y - 1) * (2 ** p)
            tmp = tmp_X * 2 + tmp_Y
            path += table[tmp]
        return path

    def path2xy(self, path, depth):
        """ç”±å›¾å—è·¯å¾„è½¬é¡¹ç›®å®šä¹‰çš„åæ ‡ã€‚
        
        ä¼ å‚æ—¶åº”ä¿è¯pathä¸ç»™å®šçˆ¬å›¾ç­‰çº§ç›¸ç¬¦ï¼Œå³ path = total_depth + target_depth 
        
        Args: E.g. : '/0/3/3/3/1/2/1/3' , 15
            path (int) : The overviewer img block path to convert.
                P.S.: The '/' in the beginning is needed
            depth (int) : The total zoom-levels for the given overviewer 
                map (total depth)."""

        in_list = map(int, path.split('/')[1:])
        X, Y = (0, 0)
        table = [1, 3, 0, 2]
        for index, value in enumerate(in_list):
            X += (table[value]//2-0.5)*2**(depth-index)  # éœ€è¦æ•´æ•°é™¤
            Y += (table[value] % 2-0.5)*2**(depth-index)
        return(int(X), int(Y))

    '''
    è‹¥å—ç½‘ç»œç­‰å½±å“æœªè·å–åˆ°å€¼ï¼Œåˆ™æ•´ä¸ªè„šæœ¬é€€å‡ºã€‚'''

    def fetchTotalDepth(self):
        # TODO: migrate to aiohttp?
        """é€å±‚çˆ¬å–å›¾å—ï¼Œæ¢æµ‹ç»™å®šçš„Overvieweråœ°å›¾ä¸€å…±å¤šå°‘å±‚

        æŒ‰ç…§å®ƒä»¬ç”Ÿæˆåœ°å›¾çš„è§„åˆ™ï¼Œç¡¬ç¼–ç å–æœ€é è¿‘åœ°å›¾åæ ‡åŸç‚¹å³ä¸Šæ–¹çš„å›¾å—ï¼Œä¾‹å¦‚ /1 /1/2 /1/2/2 â€¦â€¦(Overviewerç‰ˆæœ¬)
        è¯¥å‡½æ•°è¿”å›crawlerç±»åˆå§‹åŒ–æ‰€éœ€çš„å‚æ•°ï¼Œè‹¥å‘ç”Ÿå¼‚å¸¸åˆ™è„šæœ¬æ–‡ä»¶åº”å½“é€€å‡ºï¼Œè€Œä¸èƒ½å‘ä¸‹æ‰§è¡Œã€‚

        Args: None

        Returns:
            depth (int): The total zoom-levels for the given overviewer map."""

        errors = 0            
        configPaths = {'Overviewer' : 'overviewerConfig.js', 'Mapcrafter' : 'config.js'}
        path = '/1'
        depth = 0
        while True:  # do-while å¾ªç¯ç»“æ„çš„ä¸€ç§æ”¹å†™
            
            URL = {'Overviewer' : '{}/{}{}.jpg?c={}'.format(self.map_domain,self.map_name, path, self.timestamp),
                   'Mapcrafter' : '{}/{}/{}{}.jpg'  .format(self.map_domain,self.map_name, self.map_rotation, path)}

            try:
                print('.', end='', flush=True)  # åªè¾“å‡ºï¼Œä¸æ¢è¡Œï¼Œè¾¹çˆ¬è¾¹è¾“å‡ºã€‚
                r = requests.head(URL[self.map_type], timeout=5)
                if r.status_code == 404:
                    break
                elif r.status_code == 200:
                    depth += 1
                    direction_num = {'Overviewer' : '/2', 'Mapcrafter' : '/4'}
                    path = path+direction_num[self.map_type]
                    errors = 0
                else:
                    raise wannengError(r.status_code)
            except Exception as e:
                self.logger.error('Err {} : {}'.format(errors, e))
                errors += 1
                if errors >= 5:
                    raise e
        print()
        self.logger.info("Total zoom depth: {}".format(depth))
        return depth

    def getImgdir(self, dir):
        """è¿”å›ä¿å­˜è¯¥åœ°å›¾ä»Šæ—¥æ›´æ–°äº†çš„å›¾å—çš„æ–‡ä»¶å¤¹åœ°å€
        
        ä¿å­˜æ¯å¼ å›¾ç‰‡å‰éƒ½ä¼šæ£€æŸ¥ï¼Œè‹¥æ–‡ä»¶å¤¹ä¸å­˜åœ¨å°†è¢«åˆ›å»ºã€‚

        Args: Eg. : (.)'/images/v2_daytime'
            dir(str) : Where all images are saved.

        Yields: Eg. : (.)'/images/v2_daytime/20180202/'
            new_dir (str) : Where to save the images being crawled today."""

        new_dir = '{}/{}/'.format(dir, self.today)
        while(True):
            #time.sleep(0.03)  # ä¸æƒ³è¾“å‡ºå¤ªå¿«
            if not os.path.exists(new_dir):
                os.makedirs(new_dir)
                self.logger.info('Made directory\t./{}'.format(new_dir))
            yield new_dir

    '''ï¼Œï¼Œä¸€è½®å®Œæˆè€Œä¸æ˜¯å…ˆheadå†get'''      

    async def addNewImg(self, sess, path, URL, file_name):
        """å‘æ–‡ä»¶ç³»ç»Ÿå’Œæ›´æ–°å†å²è®°å½•ä¸­æ·»åŠ æ–°å›¾ç‰‡
        
        Args: 
            URL (str): The url of a specific image.
            file_name (str): What to save the img as.
        Returns:
            ret_msg (str): The log message of the very image."""
        # æŠŠ save_in ä¼ å…¥ï¼ŒæŠŠupdate_history æåˆ°selfé‡Œï¼Œå°±å¯å°†æ­¤æ–¹æ³•æå‡ºä¸Šä¸€å±‚æ–¹æ³•å»ã€‚
        # ä¸ºé€‚åº”æ•°æ®åº“æ‰€åšçš„è§„åˆ’ï¼šå°†update_historyçš„èµ‹å€¼è¡Œä¸ºæ”¹æˆæ•°æ®åº“insertæ“ä½œä½†ä¸commit
        response = await self.downloadImage(sess, URL)
        self.update_history[file_name] = (
            [{'Save_in': self.save_in.next(), 'ETag': response['headers']['ETag']}])
        with open(self.save_in.next()+file_name, 'wb') as f:
            f.write(response['image'])
            f.close()
        ret_msg = 'Add\t{}.jpg as {}'.format(path, file_name)
        return ret_msg 

    async def processBySHA1(self, sess, URL, response, file_name, coord):
        """ä¸‹è½½å›¾å—å¹¶æ ¹æ®æ‘˜è¦æ¥å¤„ç†æ–‡ä»¶
        
        é€‚ç”¨äºæ–°å¢å›¾ç‰‡(Add)åŠç«™ç‚¹æœ€æ–°å›¾ç‰‡å’Œæœ¬åœ°ä¿å­˜çš„æœ€æ–°å›¾ç‰‡ETagä¸åŒçš„æ—¶å€™(nMod, upd, rep)
        
        Args:
            sess: aiohttp.ClientSession.
            URL (str) : The url of a specific image.
            response : The response of head(url).
            file_name (str): What to save the img as.
            coord(set(x,y)): x and y coordinate of the image patch.
        
        Returns:
            ret_msg (str): The log message of the very image."""

        #æ•°æ®åº“é¢„æƒ³ï¼š
        #   TODO In_stock_latest æ‰€å¯¹åº”å›¾ç‰‡çš„è·å–æ–¹å¼è¦å˜æ›´ï¼Œ
        #   é¢å‘æ•°æ®åº“å’Œossåšä¸€ä¸ªæ–‡ä»¶è®¿é—®æ–¹æ³•ï¼Œèƒ½è¯»/å†™/è·å–è·¯å¾„?/åˆ›å»ºå°šä¸å­˜åœ¨çš„ç›®å½•ï¼Œç½‘ç»œé”™è¯¯åˆ™ç›´æ¥raiseä¸ç”¨catch
        #
        #   processBySha1 æ–¹æ³•åæ”¹ä¸º selectiveSaveImg()
        #   æ–°æ–¹æ³•ï¼šgetLatestUpdatePath(), å¦‚filenameåœ¨DBæ— è®°å½•åˆ™è¿”å›''ã€‚
        #   æ–°æ–¹æ³•ï¼šaddCrawlRecord(), 
        #       åœ¨Repå·¥å†µä¸‹è¦å°†å·²æœ‰çš„ç›¸åŒ(æ—¥æœŸï¼Œæ–‡ä»¶å)è®°å½•ç½®ä¸ºæ— æ•ˆï¼š
        #       æ–°æ–¹æ³•ï¼š deactivateCrawlRecord()
        

        DL_img = await self.downloadImage(sess, URL)
        DL_img = DL_img['image']
        In_Stock_Latest = self.getLatestUpdatePath(file_name) + file_name

        with open(In_Stock_Latest, 'rb') as Prev_img:
            # SHA1ä¸ä¸€è‡´ï¼Œå–»ç¤ºå›¾ç‰‡å‘ç”Ÿäº†å®è´¨æ€§ä¿®æ”¹ (Upd, Rep)
            if hashlib.sha1(Prev_img .read()).hexdigest() != hashlib.sha1(DL_img).hexdigest():
                # åŒä¸€å¤©å†…ä¸¤æ¬¡æŠ“åˆ°çš„å›¾ç‰‡å‘ç”Ÿäº†åå·®ï¼Œæ›¿æ¢æ‰æœ¬åœ°åŸæ¥çš„æœ€æ–°å›¾ç‰‡å’Œæ›´æ–°è®°å½•
                # èƒ½å¦ç”¨ä¸€æ¡é€»è¾‘å®ç°â€œå¦‚æœè¦æ’å…¥çš„è®°å½•å·²å­˜åœ¨åˆ™ä¿®æ”¹åº“ä¸­è®°å½•â€ï¼Ÿ

                if self.getLatestUpdateDate(file_name) == self.today:                     
                    del self.update_history[file_name][-1]
                    # self.deactivateCrawlRecord(file_name, self.today) 
                    self.statistics.plus('Replace')
                    ret_msg = 'Rep\t{}'.format(file_name)
                else:
                    self.statistics.plus('Update')
                    ret_msg = 'Upd\t{}'.format(file_name)
                self.update_history[file_name].append(
                    {'Save_in': self.save_in.next(), 'ETag': response.headers['ETag']})
                #self.addCrawlRecord(
                #    file_name, self.today, response.headers['ETag'], 
                #    self.target_depth, coord[0], coord[1], self.save_in.next())
                with open(self.save_in.next()+file_name, 'wb') as f:
                        f.write(DL_img)
                        f.close()
            else:
                # SHA1ä¸€è‡´ï¼Œå›¾ç‰‡æ— å®è´¨æ€§å˜åŒ–ï¼Œåˆ™å¿½ç•¥è¯¥ä¸åŒ
                self.statistics.plus('unModded')
                ret_msg = 'nMOD\t{}'.format(file_name)
            return ret_msg

    async def visitPath(self, sess, path):  
        """æŠ“å–å•å¼ å›¾ç‰‡å¹¶å¯¹å“åº”è¿›è¡Œå¤„ç†çš„å·¥äºº
        
        å¯¹æ¯å¼ å›¾ç‰‡è¿›è¡Œæœ€å¤š5æ¬¡ä¸‹è½½å°è¯•ï¼Œå¦‚æœè¿˜æ˜¯ä¸‹ä¸æ¥å°±æ”¾å¼ƒè¿™å¼ å›¾ç‰‡
        
        Args: Eg. : /0/3/3/3/3/3/3/2/3/2/3/1
            path (str): The overviewer img block path to download.

        Returns:
            ret_msg (str): The log message of the very image."""
        #é€‚åº”æ•°æ®åº“çš„æ”¹é€ ï¼š
        #   replaceçš„æ•°æ®åº“å¤„ç†é€»è¾‘è¦æ”¹å˜ï¼Œä¸èƒ½åˆ ä¸Šä¸€å¤©çš„æœ€æ–°æ›´æ–°è®°å½•äº†ã€‚

        URL = '{}/{}{}.jpg?c={}'.format(self.map_domain,
                                        self.map_name, path, self.timestamp)
        tryed_time = 0
        # save_in æœ¬ä¸å±äºè¿™é‡Œï¼Œæƒå®œä¹‹è®¡ï¼Œå¾…æ–‡ä»¶è®¿é—®æ–¹æ³•å®Œå·¥åç§»é™¤ã€‚

        while True:
            visitpath_status = 'none'
            try:
                ret_msg = 'none..'
                async with self.crawlJob_semaphore:
                    async with sess.head(URL, timeout = 5) as response:
                        r = response
                    #r = requests.head(URL, timeout=5)
                    
                    # 404--å›¾å—ä¸å­˜åœ¨
                    if r.status == 404:
                        self.statistics.plus('404')
                        ret_msg = '404\t{}'.format(path)
                    # 200--OK
                    elif r.status == 200:
                        XY = self.path2xy(path, self.total_depth)
                        file_name = '{}_{}_{}.jpg'.format(
                            self.target_depth, XY[0], XY[1])
                        visitpath_status = 'filename set'
                        '''
                        # åº“é‡Œæ— è¯¥å›¾--Add
                        if file_name not in self.update_history:
                            visitpath_status = 'To add img'
                            ret_msg = await self.addNewImg(sess, path, URL, file_name)
                            self.latest_ETag[file_name] = {'ETag' : r.headers['ETag']}
                            self.statistics.plus('Added')
                            visitpath_status = 'img added'
                        '''
                        # åº“é‡Œæœ‰è¯¥å›¾ç‰‡
                        #else:
                        # ETagä¸ä¸€è‡´--ä¸¢ç»™ä¸‹ä¸€çº§å¤„ç†
                        # TODO: self.getLatestSavedETag(file_name) -> str
                        try:
                            if r.headers['ETag'] != self.latest_ETag[file_name]['ETag']: 
                                # BUG: ğŸ‘†latest_etag ä¸­æ²¡æœ‰éƒ¨åˆ†å›¾å—ï¼Œè€Œupdate_historyé‡Œå´æœ‰ã€‚
                                # è¿™æ˜¯ç”±äºé‚£äº›å›¾å—å‡åœ¨åœ°å›¾è¾¹ç¼˜ä¸”latest_etagä½œä¸ºç‹¬ç«‹æ–‡ä»¶å»ºç«‹è¾ƒæ™šï¼Œ
                                # å»ºç«‹åå›¾å—å°±ä¸€ç›´æ²¡æ›´æ–°äº†ã€‚
                                # å»ºè®®åˆ é™¤update_historyä¸­çš„é‚£äº›å›¾å—å¹¶æ ¡éªŒä¸¤ä¸ªæ•°æ®æ–‡ä»¶ä¸­çš„é”®ä¸€è‡´æ€§ã€‚
                                visitpath_status = 'ETag inconsistent'
                                ret_msg = await self.processBySHA1(sess, URL, r, file_name, XY)
                            # ETagä¸€è‡´--åªå‡ºä¸ªlog
                            else:
                                visitpath_status = 'ETag consistent'
                                self.statistics.plus('Ignore')
                                ret_msg = 'Ign\t{}'.format(file_name)
                        except KeyError as e: 
                            # update_historyä¸­çš„éƒ¨åˆ†å›¾å—é”®åœ¨latest_etagä¸­æ²¡æœ‰ï¼Œæ˜¯å†å²é—ç•™é—®é¢˜ã€‚
                            # åœ¨è¿™catchæ‰å¼‚å¸¸ï¼Œåé¢ä¸€è¡Œä»£ç å¥½æ·»åŠ æ­£ç¡®çš„etagã€‚
                            self.logger.error(e)
                            self.logger.error('{} don\'t show up in latest_ETag but shows in '.format(path))
                            self.latest_ETag[file_name] = {'ETag' : self.update_history[file_name][-1]['ETag']}
                            self.logger.error('Copied ETag from update_history to latest_ETag for {}'.format(file_name))
                        self.latest_ETag[file_name]['ETag'] = r.headers['ETag']
                self.logger.warn(ret_msg) if 'Fail' in ret_msg or 'Rep' in ret_msg else self.logger.info(ret_msg)
                return ret_msg
            except (KeyboardInterrupt) as e:
                raise e
            # ç½‘ç»œé‡åˆ°é—®é¢˜ï¼Œé‡è¯•æœ€å¤š5æ¬¡
            # BUG: å¼‚å¸¸å¤„ç†é¡ºåºå¯¼è‡´éƒ¨åˆ†çˆ¬å–çŠ¶æ€ï¼ˆä¾‹å¦‚ï¼šç½‘ç»œå¼‚å¸¸çˆ¬å–æŠ¥é”™æŠ¥é”™ï¼‰ä¸‹çš„ctrl+cæ— æ•ˆã€‚
            except Exception as e:
                #if e is keyboardexception: raise;
                self.logger.error(
                    'No.{} for\t{}\t{}'.format(tryed_time, path, e))
                self.logger.error('at: {}'.format(visitpath_status))
                tryed_time += 1
                if tryed_time >= 5:
                    self.statistics.plus('Fail')
                    ret_msg = 'Fail\t{}'.format(path)
                    self.logger.warn(ret_msg) if 'Fail' in ret_msg or 'Rep' in ret_msg else self.logger.info(ret_msg)
                    return ret_msg
            # using `finally` here will break the 5-time-tolerant `while`-loop.

    async def visitPaths(self, paths):
        self.crawlJob_semaphore = asyncio.Semaphore(self.max_threads) # TODO: rename 'threads' as 'workers'
        async with aiohttp.ClientSession(read_bufsize = 2 ** 18) as sess:
            await asyncio.gather(*[self.visitPath(sess, path) for path in paths])
            


    def runsDaily(self):
        """æ¯æ—¥è¿è¡Œçš„æŠ“å›¾å­˜å›¾å‡½æ•°ï¼Œä»¥å•ä¸ªovervieweråœ°å›¾ä¸ºèŒƒå›´ï¼ŒæŠ“å–å¹¶æ›´æ–°åº“ä¸­çš„å›¾ç‰‡ã€‚
        
        ç¬¬ä¸€æ¬¡è¿è¡Œåˆ›å»ºè·¯å¾„å’Œæ•°æ®æ–‡ä»¶ï¼Œå…¨é‡ä¸‹/å­˜å›¾ç‰‡ï¼Œ
        åç»­æ¯æ¬¡åªä¸‹è½½ETagå˜åŠ¨çš„å›¾ç‰‡å¹¶ä¿å­˜å…¶ä¸­SHA1å˜åŠ¨çš„å›¾ç‰‡(çº¦å å‰è€…çš„1/3?)"""

        bot = Bot(token = CrConf.telegram_bot_key )

        self.statistics = counter() # ç»Ÿè®¡æŠ“å›¾çŠ¶æ€
        self.update_history = {}  # æ›´æ–°å†å²
        self.latest_ETag = {} # æ¯ä¸ªåŒºå—çš„æœ€æ–°ETag


        save_in = self.getImgdir(self.image_folder)
        self.save_in = threadsafe_generator(save_in)

        # è¯»å–å›¾å—æ›´æ–°å²ï¼Œè‹¥æ–‡ä»¶ä¸å­˜åœ¨åˆ™è¿å¸¦æ‰€è¿°ç›®å½•ä¸€åŒåˆ›å»ºã€‚
        # -> Get a DB object() ?
        self.sqliteConnection = self.prepareDBConnection()

        to_crawl = self.makePath(
            self.crawl_zones, self.target_depth)  # ç”Ÿæˆè¦æŠ“å–çš„å›¾ç‰‡åæ ‡


        # ç»´æŠ¤ä¸€ä¸ªæŠ“å›¾åç¨‹(æ± ?)
        loop = asyncio.get_event_loop()
        try:
            loop.run_until_complete(self.visitPaths(to_crawl))
        except KeyboardInterrupt:
            self.logger.warn('User pressed ctrl+c.')
            self.logger.warn('Will exit when other threads return.')
            return 0
        finally:
            self.updateDBDates()
            if not self.dry_run:
                self.sqliteConnection.commit()
            self.sqliteConnection.close()


        # å°†ä»Šå¤©çš„æŠ“å›¾æƒ…å†µå†™å›æ›´æ–°å†å²æ–‡ä»¶
        # TODO è‹¥ç”¨æµ‹è¯•ä»£ç è¯»å–ç”Ÿäº§åº“åˆ™è¦å…ˆå¤åˆ¶ç”Ÿäº§åº“åˆ°æµ‹è¯•ç¯å¢ƒã€‚
        self.logger.debug('Start dumping json at {}'.format(time.time()))
        with open('{}/update_history.json'.format(self.data_folder), 'w') as f:
            json.dump(self.update_history, f, indent=2, sort_keys=True)
            self.logger.debug('update_history dumped at {}'.format(time.time()))
        with open('{}/latest_ETag.json'.format(self.data_folder), 'w') as f:
            json.dump(self.latest_ETag,f,indent=2, sort_keys=True)
            self.logger.debug('latest_ETag dumped at {}'.format(time.time()))
        try:
            bot.send_message(CrConf.telegram_msg_recipient, 'Crawl result {} for {} : \n{}'.format(self.today,self.map_name,str(self.statistics)))
        except Exception :
            self.logger.warning('Telegram bot failed sending {} statistics!'.format(self.map_name))

def main():
    try:
        for map_name, map_conf in map_list.items():
            if map_conf.enable_crawl == True:
                cr = crawler(map_conf, noFetch=True)
                cr.runsDaily()
                if map_conf.last_total_depth != cr.total_depth:
                    map_conf.last_total_depth = cr.total_depth
            else: 
                print("skipping map {}".format(map_name))
        #f.seek(0)
        #json.dump(configs, f, indent=2, sort_keys=True)
        #f.truncate()
    except Exception as e:
        print(e)        
        with open('{}/log/errors.txt'.format(CrConf.data_folders),'a+') as f:
            print(str(e),file = f)
        bot = Bot(token = CrConf.telegram_bot_key )
        bot.send_message(CrConf.telegram_msg_recipient,'Something went wrong, see logs/errors.txt for detail')



if __name__ == '__main__':
    main()
